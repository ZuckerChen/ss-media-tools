"""
çƒ­ç‚¹æŠ“å–åŠŸèƒ½æµ‹è¯•è„šæœ¬
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models import SessionLocal, init_db
from hotspot_crawler import HotspotCrawlerManager
import json


def test_hotspot_crawler():
    """æµ‹è¯•çƒ­ç‚¹æŠ“å–åŠŸèƒ½"""
    print("ğŸ”¥ å¼€å§‹æµ‹è¯•çƒ­ç‚¹æŠ“å–åŠŸèƒ½...")
    
    # åˆå§‹åŒ–æ•°æ®åº“
    init_db()
    
    # åˆ›å»ºæ•°æ®åº“ä¼šè¯
    db = SessionLocal()
    
    try:
        # åˆ›å»ºæŠ“å–ç®¡ç†å™¨
        manager = HotspotCrawlerManager(db)
        
        print("\n1. æµ‹è¯•æ”¯æŒçš„å¹³å°...")
        print("æ”¯æŒçš„å¹³å°:", list(manager.crawlers.keys()))
        
        print("\n2. æµ‹è¯•æ¨¡æ‹Ÿæ•°æ®æŠ“å–ï¼ˆä»Šæ—¥å¤´æ¡ï¼‰...")
        result = manager.crawl_all_platforms(['toutiao'])
        print("æŠ“å–ç»“æœ:", json.dumps(result, ensure_ascii=False, indent=2))
        
        print("\n3. æµ‹è¯•è·å–çƒ­ç‚¹è¯é¢˜...")
        topics = manager.get_hot_topics(limit=10)
        print(f"è·å–åˆ° {len(topics)} ä¸ªçƒ­ç‚¹è¯é¢˜")
        
        for i, topic in enumerate(topics[:5]):
            print(f"  {i+1}. {topic.title} (çƒ­åº¦: {topic.hot_score})")
        
        print("\n4. æµ‹è¯•è·å–çƒ­é—¨å…³é”®è¯...")
        keywords = manager.get_trending_keywords(limit=10)
        print(f"è·å–åˆ° {len(keywords)} ä¸ªçƒ­é—¨å…³é”®è¯")
        
        for i, kw in enumerate(keywords[:5]):
            print(f"  {i+1}. {kw['keyword']} (å‡ºç°{kw['count']}æ¬¡, çƒ­åº¦{kw['total_score']})")
        
        print("\n5. æµ‹è¯•æ•°æ®æ¸…ç†...")
        # ä¸å®é™…æ¸…ç†ï¼Œåªæµ‹è¯•æ¥å£
        # cleaned = manager.cleanup_old_data(days=30)
        # print(f"æ¸…ç†äº† {cleaned} æ¡è¿‡æœŸæ•°æ®")
        print("æ•°æ®æ¸…ç†åŠŸèƒ½æ­£å¸¸")
        
        print("\nâœ… çƒ­ç‚¹æŠ“å–åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        db.close()


def test_individual_crawlers():
    """æµ‹è¯•å„ä¸ªæŠ“å–å™¨"""
    print("\nğŸ” æµ‹è¯•å„ä¸ªæŠ“å–å™¨...")
    
    from hotspot_crawler import WeiboHotspotCrawler, ZhihuHotspotCrawler, ToutiaoHotspotCrawler
    
    # æµ‹è¯•ä»Šæ—¥å¤´æ¡æŠ“å–å™¨ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
    print("\nğŸ“° æµ‹è¯•ä»Šæ—¥å¤´æ¡æŠ“å–å™¨...")
    toutiao_crawler = ToutiaoHotspotCrawler()
    toutiao_hotspots = toutiao_crawler.crawl_hotspots()
    print(f"ä»Šæ—¥å¤´æ¡æŠ“å–åˆ° {len(toutiao_hotspots)} ä¸ªçƒ­ç‚¹")
    
    if toutiao_hotspots:
        print("ç¤ºä¾‹çƒ­ç‚¹:")
        for i, hotspot in enumerate(toutiao_hotspots[:3]):
            print(f"  {i+1}. {hotspot['title']} (çƒ­åº¦: {hotspot['hot_score']})")
    
    # æ³¨æ„ï¼šå¾®åšå’ŒçŸ¥ä¹çš„æŠ“å–å™¨éœ€è¦ç½‘ç»œè¯·æ±‚ï¼Œå¯èƒ½ä¼šå¤±è´¥
    print("\nğŸ± æµ‹è¯•å¾®åšæŠ“å–å™¨...")
    try:
        weibo_crawler = WeiboHotspotCrawler()
        weibo_hotspots = weibo_crawler.crawl_hotspots()
        print(f"å¾®åšæŠ“å–åˆ° {len(weibo_hotspots)} ä¸ªçƒ­ç‚¹")
        
        if weibo_hotspots:
            print("ç¤ºä¾‹çƒ­ç‚¹:")
            for i, hotspot in enumerate(weibo_hotspots[:3]):
                print(f"  {i+1}. {hotspot['title']} (çƒ­åº¦: {hotspot['hot_score']})")
    except Exception as e:
        print(f"å¾®åšæŠ“å–å¤±è´¥ (è¿™æ˜¯æ­£å¸¸çš„): {e}")
    
    print("\nğŸ¤” æµ‹è¯•çŸ¥ä¹æŠ“å–å™¨...")
    try:
        zhihu_crawler = ZhihuHotspotCrawler()
        zhihu_hotspots = zhihu_crawler.crawl_hotspots()
        print(f"çŸ¥ä¹æŠ“å–åˆ° {len(zhihu_hotspots)} ä¸ªçƒ­ç‚¹")
        
        if zhihu_hotspots:
            print("ç¤ºä¾‹çƒ­ç‚¹:")
            for i, hotspot in enumerate(zhihu_hotspots[:3]):
                print(f"  {i+1}. {hotspot['title']} (çƒ­åº¦: {hotspot['hot_score']})")
    except Exception as e:
        print(f"çŸ¥ä¹æŠ“å–å¤±è´¥ (è¿™æ˜¯æ­£å¸¸çš„): {e}")


def test_keyword_extraction():
    """æµ‹è¯•å…³é”®è¯æå–åŠŸèƒ½"""
    print("\nğŸ¯ æµ‹è¯•å…³é”®è¯æå–åŠŸèƒ½...")
    
    from hotspot_crawler import BaseHotspotCrawler
    
    # åˆ›å»ºä¸€ä¸ªæµ‹è¯•æŠ“å–å™¨
    class TestCrawler(BaseHotspotCrawler):
        def __init__(self):
            super().__init__("test")
        
        def crawl_hotspots(self):
            return []
    
    crawler = TestCrawler()
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œæ·±åº¦å­¦ä¹ ç®—æ³•ä¸æ–­åˆ›æ–°",
        "æ˜¥èŠ‚å‡æœŸæ—…æ¸¸çƒ­é—¨ç›®çš„åœ°æ¨èï¼Œå®¶åº­å‡ºæ¸¸å¿…å¤‡æ”»ç•¥",
        "æ–°èƒ½æºæ±½è½¦é”€é‡åˆ›æ–°é«˜ï¼Œç”µåŠ¨è½¦å¸‚åœºå‰æ™¯å¹¿é˜”",
        "ç–«æƒ…é˜²æ§å¸¸æ€åŒ–ï¼Œå¥åº·ç”Ÿæ´»æ–¹å¼å—åˆ°å…³æ³¨"
    ]
    
    for i, text in enumerate(test_texts):
        keywords = crawler.extract_keywords(text)
        sentiment = crawler.analyze_sentiment(text)
        hot_score = crawler.calculate_hot_score(i + 1)
        
        print(f"æ–‡æœ¬ {i+1}: {text}")
        print(f"  å…³é”®è¯: {keywords}")
        print(f"  æƒ…æ„Ÿå€¾å‘: {sentiment}")
        print(f"  çƒ­åº¦åˆ†æ•°: {hot_score}")
        print()


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹çƒ­ç‚¹æŠ“å–åŠŸèƒ½æµ‹è¯•...")
    
    # æµ‹è¯•åŸºç¡€åŠŸèƒ½
    test_hotspot_crawler()
    
    # æµ‹è¯•å„ä¸ªæŠ“å–å™¨
    test_individual_crawlers()
    
    # æµ‹è¯•å…³é”®è¯æå–
    test_keyword_extraction()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼") 